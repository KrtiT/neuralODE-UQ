{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPhkiuG3TqYxI1ggctQ/xv9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"IP0_g6DYC9T4"},"outputs":[],"source":["######## Implementing Draft 2 of the LR Scheduler ########\n","######## (Note from Krti: Caleb, feel free to merge whatever parts of this works, ########\n","######## into the LR Scheduler cell above.)  ########\n","\n","# If this is not helpful, please feel free to not use!  But let's leave this here for now in case we want to pull anything later.\n","\n","scheduler = LinearLR(optimizer_ODEnet, start_factor=0.5, total_iters=NUMBER_EPOCHS * len(train_loader))\n","\n","criterion = torch.nn.MSELoss()\n","ode_loss_hist = []\n","ode_test_loss_hist = []\n","ODEnet_best = None\n","\n","print('ODENet Training')\n","for epoch in range(1, NUMBER_EPOCHS + 1):  # Adjusted the range to include the last epoch\n","    for batch_idx, (inputs, targets) in enumerate(train_loader):\n","        optimizer_ODEnet.zero_grad()\n","        outputs = ODEnet(inputs, h=dt, dt=dt, method=method)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer_ODEnet.step()\n","        ode_loss_hist.append(loss.item())\n","        scheduler.step()  # How about: Call the scheduler after each optimization step?\n","\n","    # Evaluate test loss after each epoch\n","    for batch_idx, (inputs, targets) in enumerate(test_loader):\n","        outputs = ODEnet(inputs, h=dt, dt=dt, method=method)\n","        test_loss = criterion(outputs, targets)\n","        ode_test_loss_hist.append(test_loss.item())\n","\n","    if test_loss.item() <= min(ode_test_loss_hist):\n","        print(f'*** Found new best ODEnet (Epoch: {epoch}, Test Loss: {test_loss.item()}')\n","        ODEnet_best = copy.deepcopy(ODEnet)\n","\n","    if epoch % 10 == 0:\n","        print(f'Epoch: {epoch}, Loss: {loss.item()}')\n","\n","return ODEnet, ODEnet_best, ode_loss_hist\n","\n","\n","# In this code:\n","# I've defined a custom learning rate scheduler LinearLR that decreases the learning rate linearly over the specified number of epochs.\n","# Inside the training loop, I've added scheduler.step() after each optimization step to update the learning rate.\n","# I've moved the evaluation of the test loss to be performed after each epoch, so you can track the performance more accurately.\n","# The best model based on test loss is saved after each epoch if it's better than the previous best.\n","# The learning rate scheduler is used to adjust the learning rate during training."]}]}