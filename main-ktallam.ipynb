import os
import numpy as np
import scipy as sci

from scipy.integrate import solve_ivp

import matplotlib.pyplot as plt

import torch
import torch.optim as optim
import copy

from scipy.special import ellipj, ellipk
from torch.optim.lr_scheduler import LinearLR

BATCH_SIZE = 50 # 150
WEIGHT_DECAY = 0
LEARNING_RATE = 5e-3 # 1e-2
NUMBER_EPOCHS = 1000 # 4000

def set_seed(seed=10):
    """Set one seed for reproducibility."""
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)

def create_data(tmax=20, dt=1, theta0=2.0):
    """Solution for the nonlinear pendulum in theta space."""
    t = np.arange(0, tmax, dt)
    S = np.sin(0.5 * theta0)
    K_S = ellipk(S**2)
    omega_0 = np.sqrt(9.81)
    sn, cn, dn, ph = ellipj(K_S - omega_0 * t, S**2)
    theta = 2.0 * np.arcsin(S * sn)
    d_sn_du = cn * dn
    d_sn_dt = -omega_0 * d_sn_du
    d_theta_dt = 2.0 * S * d_sn_dt / np.sqrt(1.0 - (S * sn)**2)
    return np.stack([theta, d_theta_dt], axis=1)

def create_dataloader(x, batch_size=BATCH_SIZE):
    dataset = torch.utils.data.TensorDataset(
        torch.tensor(np.asarray(x[0:-1]), dtype=torch.double),
        torch.tensor(np.asarray(x[1::]), dtype=torch.double),
    )

    train_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE)
    test_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)
    return train_loader, test_loader

def euler_step_func(f, x, dt):
    """The 'forward' Euler, a one stage Runge Kutta."""
    k1 = f(x)
    x_out = x + dt * k1
    return x_out

def rk4_step_func(f, x, dt):
    """The 'classic' RK4, a four stage Runge Kutta, O(Dt^4)."""
    k1 = f(x)
    x1 = x + 0.5 * dt * k1
    k2 = f(x1)
    x2 = x + 0.5 * dt * k2
    k3 = f(x2)
    x3 = x + dt * k3
    k4 = f(x3)
    x_out = x + dt * (1.0 / 6.0 * k1 + 1.0 / 3.0 * k2 + 1.0 / 3.0 * k3 + 1.0 / 6.0 * k4)
    return x_out

def shallow(in_dim, hidden, out_dim, Act=torch.nn.Tanh):
    """Just make a shallow network. This is more of a macro."""
    return torch.nn.Sequential(
        torch.nn.Linear(in_dim, hidden),
        Act(),
        torch.nn.Linear(hidden, out_dim),
    )

class ShallowODE(torch.nn.Module):
    """A basic shallow network that takes in a t as well"""

    def __init__(self, in_dim, out_dim, hidden=10, Act=torch.nn.Tanh, dt=None, method='euler'):
        super(ShallowODE, self).__init__()
        self.net = shallow(in_dim, hidden, out_dim, Act=Act)
        self.dt = dt
        self.method = method

    def forward(self, x):
        if self.method == 'euler':
            x = euler_step_func(self.net, x, self.dt)
            return x
        elif self.method == 'rk4':
            x = rk4_step_func(self.net, x, self.dt)
            return x

def train(ODEnet, train_loader, test_loader, lr=LEARNING_RATE, wd=WEIGHT_DECAY, method='rk4', dt=0.1):
    optimizer_ODEnet = optim.Adam(ODEnet.parameters(), lr=lr, weight_decay=wd)
    scheduler = LinearLR(optimizer_ODEnet, start_factor=0.5, total_iters=4)
    criterion = torch.nn.MSELoss()
    ode_loss_hist = []
    ode_loss_ave_hist = []
    ode_test_loss_hist = []
    ode_test_loss_ave_hist = []
    ODEnet_best = None

    # set integrator and time step methods
    ODEnet.dt = dt
    ODEnet.method = method

    print('ODENet Training')
    for epoch in range(1, NUMBER_EPOCHS + 1):
        loss_ave = 0.0
        ODEnet.train()
        for batch_idx, (inputs, targets) in enumerate(train_loader):
            optimizer_ODEnet.zero_grad()
            outputs = ODEnet(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer_ODEnet.step()
            loss_ave += loss.item()
            ode_loss_hist.append(loss.item())
        loss_ave /= len(train_loader)
        ode_loss_ave_hist.append(loss_ave)

        ODEnet.eval()
        test_loss_ave = 0.0
        with torch.no_grad():
            for batch_idx, (inputs, targets) in enumerate(test_loader):
                outputs = ODEnet(inputs)
                test_loss = criterion(outputs, targets)
                test_loss_ave += test_loss.item()
        test_loss_ave /= len(test_loader)
        ode_test_loss_ave_hist.append(test_loss_ave)

        if test_loss_ave < min(ode_test_loss_ave_hist, default=float('inf')):
            print(f'*** Found new best ODEnet (Epoch: {epoch}, Test Loss: {test_loss_ave})')
            ODEnet_best = copy.deepcopy(ODEnet)

        if epoch % 10 == 0:
            print(f'Epoch: {epoch}, Loss: {loss_ave}, Test Loss: {test_loss_ave}')

    return ODEnet, ODEnet_best, ode_loss_hist, ode_test_loss_ave_hist

# 1. Step the optimizer but not the scheduler within the training loop.
# 2. Evaluate test loss after each epoch.
# 3. Save the first best model based on test loss using <.
# 4. Track and save the average test loss for each epoch.
# Train Function Adjustments:
  # Optimizer Step: The optimizer steps after each batch, but the scheduler step has been commented out since stepping it each batch was not appropriate.
  # Evaluation: The model evaluation on the test set happens after all batches of training in each epoch, ensuring we evaluate on the complete test set.
  # Model Saving: The best model is saved based on the minimum average test loss, using < to ensure the first best is saved.
  # Loss Averaging: Loss values are averaged over the number of batches to give a clear picture of the epoch's performance.


