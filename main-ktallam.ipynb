import os
import numpy as np
import scipy as sci

from scipy.integrate import solve_ivp

import matplotlib.pyplot as plt

import torch
import torch.optim as optim
import copy

from scipy.special import ellipj, ellipk
from torch.optim.lr_scheduler import LinearLR

BATCH_SIZE = 50 # 150
WEIGHT_DECAY = 0
LEARNING_RATE = 5e-3 # 1e-2
NUMBER_EPOCHS = 1000 # 4000

def set_seed(seed=10):
    """Set one seed for reproducibility."""
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)

def create_data(tmax=20, dt=1, theta0=2.0):
    """Solution for the nonlinear pendulum in theta space."""
    t = np.arange(0, tmax, dt)
    S = np.sin(0.5 * theta0)
    K_S = ellipk(S**2)
    omega_0 = np.sqrt(9.81)
    sn, cn, dn, ph = ellipj(K_S - omega_0 * t, S**2)
    theta = 2.0 * np.arcsin(S * sn)
    d_sn_du = cn * dn
    d_sn_dt = -omega_0 * d_sn_du
    d_theta_dt = 2.0 * S * d_sn_dt / np.sqrt(1.0 - (S * sn)**2)
    return np.stack([theta, d_theta_dt], axis=1)

def create_dataloader(x, batch_size=BATCH_SIZE):
    dataset = torch.utils.data.TensorDataset(
        torch.tensor(np.asarray(x[0:-1]), dtype=torch.double),
        torch.tensor(np.asarray(x[1::]), dtype=torch.double),
    )

    train_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE)
    test_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)
    return train_loader, test_loader

def euler_step_func(f, x, dt):
    """The 'forward' Euler, a one stage Runge Kutta."""
    k1 = f(x)
    x_out = x + dt * k1
    return x_out

def rk4_step_func(f, x, dt):
    """The 'classic' RK4, a four stage Runge Kutta, O(Dt^4)."""
    k1 = f(x)
    x1 = x + 0.5 * dt * k1
    k2 = f(x1)
    x2 = x + 0.5 * dt * k2
    k3 = f(x2)
    x3 = x + dt * k3
    k4 = f(x3)
    x_out = x + dt * (1.0 / 6.0 * k1 + 1.0 / 3.0 * k2 + 1.0 / 3.0 * k3 + 1.0 / 6.0 * k4)
    return x_out

def shallow(in_dim, hidden, out_dim, Act=torch.nn.Tanh):
    """Just make a shallow network. This is more of a macro."""
    return torch.nn.Sequential(
        torch.nn.Linear(in_dim, hidden),
        Act(),
        torch.nn.Linear(hidden, out_dim),
    )

class ShallowODE(torch.nn.Module):
    """A basic shallow network that takes in a t as well"""

    def __init__(self, in_dim, out_dim, hidden=10, Act=torch.nn.Tanh, dt=None, method='euler'):
        super(ShallowODE, self).__init__()
        self.net = shallow(in_dim, hidden, out_dim, Act=Act)
        self.dt = dt
        self.method = method

    def forward(self, x):
        if self.method == 'euler':
            x = euler_step_func(self.net, x, self.dt)
            return x
        elif self.method == 'rk4':
            x = rk4_step_func(self.net, x, self.dt)
            return x

def train(ODEnet, train_loader, test_loader, lr=LEARNING_RATE, wd=WEIGHT_DECAY, method='rk4', dt=0.1):
    optimizer_ODEnet = optim.Adam(ODEnet.parameters(), lr=lr, weight_decay=wd)
    scheduler = LinearLR(optimizer_ODEnet, start_factor=0.5, total_iters=4)
    criterion = torch.nn.MSELoss()
    ode_loss_hist = []
    ode_loss_ave_hist = []
    ode_test_loss_hist = []
    ode_test_loss_ave_hist = []
    ODEnet_best = None

    # set integrator and time step methods
    ODEnet.dt = dt
    ODEnet.method = method

    print('ODENet Training')
    for epoch in range(1, NUMBER_EPOCHS + 1):
        loss_ave = 0.0
        ODEnet.train()
        for batch_idx, (inputs, targets) in enumerate(train_loader):
            optimizer_ODEnet.zero_grad()
            outputs = ODEnet(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer_ODEnet.step()
            loss_ave += loss.item()
            ode_loss_hist.append(loss.item())
        loss_ave /= len(train_loader)
        ode_loss_ave_hist.append(loss_ave)

        ODEnet.eval()
        test_loss_ave = 0.0
        with torch.no_grad():
            for batch_idx, (inputs, targets) in enumerate(test_loader):
                outputs = ODEnet(inputs)
                test_loss = criterion(outputs, targets)
                test_loss_ave += test_loss.item()
        test_loss_ave /= len(test_loader)
        ode_test_loss_ave_hist.append(test_loss_ave)

        if test_loss_ave < min(ode_test_loss_ave_hist, default=float('inf')):
            print(f'*** Found new best ODEnet (Epoch: {epoch}, Test Loss: {test_loss_ave})')
            ODEnet_best = copy.deepcopy(ODEnet)

        if epoch % 10 == 0:
            print(f'Epoch: {epoch}, Loss: {loss_ave}, Test Loss: {test_loss_ave}')

    return ODEnet, ODEnet_best, ode_loss_hist, ode_test_loss_ave_hist

# 1. Step the optimizer but not the scheduler within the training loop.
# 2. Evaluate test loss after each epoch.
# 3. Save the first best model based on test loss using <.
# 4. Track and save the average test loss for each epoch.
# Train Function Adjustments:
  # Optimizer Step: The optimizer steps after each batch, but the scheduler step has been commented out since stepping it each batch was not appropriate.
  # Evaluation: The model evaluation on the test set happens after all batches of training in each epoch, ensuring we evaluate on the complete test set.
  # Model Saving: The best model is saved based on the minimum average test loss, using < to ensure the first best is saved.
  # Loss Averaging: Loss values are averaged over the number of batches to give a clear picture of the epoch's performance.
# Main script execution
if __name__ == '__main__':
    # Configure parameters
    dt = 0.2
    hidden = 200
    N_points = 500
    T_MAX = N_points * dt
    noise_loc = 0.0
    noise_scale = 1.0
    SEED = 5544
    set_seed(SEED)

    # Loop over integrators
    integrators = ['euler', 'rk4'][::-1]
    for integrator in integrators:
        print(f"Testing integrator = {integrator}")

        # Load the data
        x = create_data(tmax=T_MAX, dt=dt, theta0=2.0)
        x_ood_noise = x + np.random.normal(noise_loc, noise_scale, x.shape)
        train_loader, test_loader = create_dataloader(x)
        train_ood_noise_loader, test_ood_noise_loader = create_dataloader(x_ood_noise)

        # Train OOD (different theta)
        x_ood_theta = create_data(tmax=T_MAX, dt=dt, theta0=2.5)
        train_ood_theta_loader, test_ood_theta_loader = create_dataloader(x_ood_theta)

        # Sequential split
        sequential_train_loader, sequential_test_loader = create_dataloader(x[:int(N_points * 0.80)]), create_dataloader(x[int(N_points * 0.80):])

        # Train the model
        ODEnet = ShallowODE(in_dim=2, hidden=hidden, out_dim=2, Act=torch.nn.Tanh, dt=dt, method=integrator).double()
        ODEnet, ODEnet_best, ode_loss_hist = train(ODEnet, copy.deepcopy(train_loader), copy.deepcopy(test_loader), method=integrator, dt=dt)

        use_best = True
        if use_best:
            ODEnet = ODEnet_best

        # Save model checkpoints
        checkpoint_file = f"checkpoints/ODEnet_{integrator}_dt_{dt}_hidden_{hidden}_bs_{BATCH_SIZE}_lr_{LEARNING_RATE}_wd_{WEIGHT_DECAY}_seed_{SEED}_epochs_{NUMBER_EPOCHS}.pt"
        if use_best:
            checkpoint_file = checkpoint_file.replace(".pt", "_best.pt")

        save_folder = os.path.dirname(checkpoint_file)
        if not os.path.exists(save_folder):
            os.makedirs(save_folder)

        torch.save({"model_state_dict": ODEnet.state_dict()}, checkpoint_file)
        print(f"[+] {checkpoint_file}")

        # Evaluate the model
        hs = [0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.5, 1, 2, 3, 4, 5, 10]
        error = []
        for h in hs:
            T_MAX = N_points * dt
            x = create_data(tmax=T_MAX, dt=h)
            _, eval_loader = create_dataloader(x)

            target_list = []
            output_list = []
            for batch_idx, (inputs, targets) in enumerate(eval_loader):
                ODEnet.dt = h
                ODEnet.method = integrator
                outputs = ODEnet(inputs)
                output_list.append(outputs.detach().numpy())
                target_list.append(targets.numpy())

            error.append(np.mean(np.linalg.norm(np.vstack(output_list) - np.vstack(target_list), axis=1)**2))

        error = np.vstack(error)

        plt.plot(hs, error, 'o--', label=integrator)
        plt.yscale('log')
        plt.xscale('log')
        plt.legend(fontsize=18)
        plt.tight_layout()
        plt.show()

# In this updated section of the script:
        # Loop over Integrators: A loop iterates through the specified integrators ('euler' and 'rk4') and tests each one.
        # Sequential Split: A sequential split of the data is performed for training and testing.
        # Save Model Checkpoints: The model's state dictionary is saved to a specified checkpoint file.
        # Evaluate the Model: The model is evaluated for various time steps h and the results are plotted.

import torch
from torch.autograd import grad
from scipy.sparse.linalg import LinearOperator, eigsh
import numpy as np

def get_hessian_eigenvectors(model, compute_loss_fn, train_data_loader, num_batches, device, n_top_vectors, param_extract_fn=None):
    """
    Calculate the top eigenvalues and eigenvectors of the Hessian matrix for a given model and loss function.

    Args:
    - model: A PyTorch model.
    - compute_loss_fn: A function to compute the loss.
    - train_data_loader: A PyTorch DataLoader with training data.
    - num_batches: Number of batches to use for the Hessian calculation.
    - device: The device (CPU or GPU) for computation.
    - n_top_vectors: Number of top eigenvalues/eigenvectors to return.
    - param_extract_fn: A function that takes a model and returns a list of parameters to compute the Hessian with respect to. If None, use all parameters.

    Returns:
    - eigenvalues: A numpy array of the top eigenvalues, arranged in increasing order.
    - eigenvectors: A numpy array of the top eigenvectors, arranged in increasing order, shape (n_top_vectors, num_params).
    """
    
    if param_extract_fn is None:
        param_extract_fn = lambda x: x.parameters()

    num_params = sum(p.numel() for p in param_extract_fn(model))

    subset_images, subset_labels = [], []
    for batch_idx, (images, labels) in enumerate(train_data_loader):
        if batch_idx >= num_batches:
            break
        subset_images.append(images.to(device))
        subset_labels.append(labels.to(device))
    
    subset_images = torch.cat(subset_images)
    subset_labels = torch.cat(subset_labels)

    def hessian_vector_product(vector):
        model.zero_grad()
        grad_params = grad(compute_loss_fn(model, subset_images, subset_labels), param_extract_fn(model), create_graph=True)
        flat_grad = torch.cat([g.view(-1) for g in grad_params])
        grad_vector_product = torch.sum(flat_grad * vector)
        hvp = grad(grad_vector_product, param_extract_fn(model), retain_graph=True)
        return torch.cat([g.contiguous().view(-1) for g in hvp])

    def matvec(v):
        v_tensor = torch.tensor(v, dtype=torch.float32, device=device)
        return hessian_vector_product(v_tensor).cpu().detach().numpy()

    linear_operator = LinearOperator((num_params, num_params), matvec=matvec)
    eigenvalues, eigenvectors = eigsh(linear_operator, k=n_top_vectors, tol=0.001, which='LM', return_eigenvectors=True)
    eigenvectors = np.transpose(eigenvectors)
    
    return eigenvalues, eigenvectors

# Changes made to original script:
        # Added docstrings and comments for better understanding.
        # Used default argument for param_extract_fn to be None and handled it within the function.
        # Ensured consistent formatting and indentation.
        # Simplified the logic to collect subsets of images and labels.
        # Added meaningful comments to describe the purpose of each part of the code.
